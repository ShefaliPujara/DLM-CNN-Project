{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a20a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eebf5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self):\n",
    "        # Original 50 MS Word fonts (unchanged)\n",
    "        self.fonts = [\n",
    "            'arial.ttf', 'arialbd.ttf', 'arialbi.ttf', 'ariali.ttf',\n",
    "            'times.ttf', 'timesbd.ttf', 'timesbi.ttf', 'timesi.ttf',\n",
    "            'cour.ttf', 'courbd.ttf', 'courbi.ttf', 'couri.ttf',\n",
    "            'calibri.ttf', 'calibrib.ttf', 'calibrii.ttf', 'calibrili.ttf',\n",
    "            'cambria.ttf', 'cambriab.ttf', 'cambriai.ttf', 'cambriaz.ttf',\n",
    "            'verdana.ttf', 'verdanab.ttf', 'verdanai.ttf', 'verdanaz.ttf',\n",
    "            'georgia.ttf', 'georgiab.ttf', 'georgiai.ttf', 'georgiaz.ttf',\n",
    "            'impact.ttf', 'consola.ttf', 'constan.ttf', 'corbel.ttf',\n",
    "            'garamond.ttf', 'tahoma.ttf', 'trebuc.ttf', 'comic.ttf',\n",
    "            'wingding.ttf', 'symbol.ttf', 'bookman.ttf', 'century.ttf',\n",
    "            'courier.ttf', 'franklin.ttf', 'gillsans.ttf', 'lucida.ttf',\n",
    "            'palatino.ttf', 'rockwell.ttf', 'segoeui.ttf', 'msyh.ttf',\n",
    "            'euphemia.ttf', 'futura.ttf'\n",
    "        ]\n",
    "        \n",
    "        # Original 30 words × 2 variations = 60 samples (unchanged)\n",
    "        self.handwriting_words = [\n",
    "            \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\",\n",
    "            \"hello\", \"world\", \"python\", \"code\", \"text\", \"recognition\", \"sample\",\n",
    "            \"test\", \"data\", \"model\", \"train\", \"font\", \"hand\", \"write\", \"learn\",\n",
    "            \"neural\", \"network\", \"image\", \"process\", \"computer\", \"vision\", \"ai\"\n",
    "        ]\n",
    "        self.chars = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "        self.handwriting_variations = 2  # Strictly 2 samples per word\n",
    "\n",
    "    def generate_font_samples(self):\n",
    "        os.makedirs(\"dataset/fonts\", exist_ok=True)\n",
    "        for font_idx, font_name in enumerate(self.fonts[:50]):  # Exactly 50 fonts\n",
    "            try:\n",
    "                font_path = f\"C:/Windows/Fonts/{font_name}\"\n",
    "                if not os.path.exists(font_path):\n",
    "                    continue\n",
    "                \n",
    "                # Generate isolated characters\n",
    "                for char in self.chars:\n",
    "                    img = Image.new(\"L\", (64, 64), 255)\n",
    "                    draw = ImageDraw.Draw(img)\n",
    "                    draw.text((16, 16), char, font=ImageFont.truetype(font_path, 32), fill=0)\n",
    "                    img.save(f\"dataset/fonts/font_{font_idx}_{char}.png\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def generate_handwriting_samples(self):\n",
    "        os.makedirs(\"dataset/handwritten\", exist_ok=True)\n",
    "        sample_count = 0\n",
    "        \n",
    "        # Exactly 60 samples (30 words × 2 variations)\n",
    "        for word in self.handwriting_words[:30]:\n",
    "            for variation in range(self.handwriting_variations):\n",
    "                self._generate_handwritten_word(word, sample_count, variation)\n",
    "                sample_count += 1\n",
    "                if sample_count >= 60:\n",
    "                    break\n",
    "            if sample_count >= 60:\n",
    "                break\n",
    "\n",
    "    def _generate_handwritten_word(self, text, sample_id, variation):\n",
    "        width = len(text) * 35 + 20\n",
    "        img = Image.new(\"L\", (width, 64), 255)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Make variations maximally distinct\n",
    "        if variation == 0:  # Neat variation\n",
    "            y_offset = 0\n",
    "            size = 32\n",
    "            spacing = 28\n",
    "            slant = 0\n",
    "        else:  # Messy variation\n",
    "            y_offset = np.random.randint(-12, 12)\n",
    "            size = np.random.randint(28, 42)\n",
    "            spacing = np.random.randint(22, 34)\n",
    "            slant = np.random.randint(-20, 20)\n",
    "        \n",
    "        x_offset = 10\n",
    "        for char in text:\n",
    "            # Apply slant only to messy variation\n",
    "            if variation == 1:\n",
    "                temp_img = Image.new(\"L\", (size+10, size+10), 255)\n",
    "                temp_draw = ImageDraw.Draw(temp_img)\n",
    "                temp_draw.text((5,5), char, font=ImageFont.load_default(size=size), fill=0)\n",
    "                temp_img = temp_img.rotate(slant, expand=1, fillcolor=255)\n",
    "                img.paste(temp_img, (x_offset, 32 + y_offset))\n",
    "            else:\n",
    "                draw.text((x_offset, 32 + y_offset), char, \n",
    "                         font=ImageFont.load_default(size=size), fill=0)\n",
    "            \n",
    "            x_offset += spacing\n",
    "        \n",
    "        img = self._add_handwriting_effects(img, variation)\n",
    "        img.save(f\"dataset/handwritten/sample_{sample_id}.png\")\n",
    "\n",
    "    def _add_handwriting_effects(self, img, variation):\n",
    "        arr = np.array(img)\n",
    "        \n",
    "        # Variation-specific noise\n",
    "        if variation == 0:  # Neat\n",
    "            noise = np.random.normal(0, 3, arr.shape)\n",
    "        else:  # Messy\n",
    "            noise = np.random.normal(0, 20, arr.shape)\n",
    "        \n",
    "        arr = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Only distort messy variation\n",
    "        if variation == 1:\n",
    "            h, w = arr.shape\n",
    "            x, y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "            dx = np.random.uniform(-4, 4, (h, w)) * (y/h)\n",
    "            dy = np.random.uniform(-4, 4, (h, w)) * (y/h)\n",
    "            arr = cv2.remap(arr, np.float32(x + dx), np.float32(y + dy), \n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        return Image.fromarray(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc9f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Enhanced CNN with residual connections\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.residual1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.residual2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # Spatial Attention\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(256, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*8*8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 36)  # 26 letters + 10 digits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x + self.residual1(x)  # Residual connection\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = x + self.residual2(x)  # Residual connection\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention = self.attention(x)\n",
    "        x = x * attention\n",
    "        \n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2136d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Extreme data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomAffine(15, translate=(0.15,0.15), scale=(0.8,1.2)),\n",
    "            transforms.ColorJitter(0.3, 0.3, 0.3)\n",
    "        ], p=0.9),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    full_dataset = TextDataset(\"dataset/fonts\", \"dataset/handwritten\", train_transform)\n",
    "    train_set, test_set = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Class balancing\n",
    "    class_counts = np.bincount([label for _, label in full_dataset])\n",
    "    class_weights = 1. / torch.Tensor(class_counts)\n",
    "    sample_weights = class_weights[[label for _, label in train_set]]\n",
    "    \n",
    "    # Weighted sampler\n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "        sample_weights, len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=64, sampler=sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=64)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TextRecognitionModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    \n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"text_recognition_model.pth\")\n",
    "            if best_acc >= 90:  # Early exit if target reached\n",
    "                print(\"Target accuracy reached!\")\n",
    "                break\n",
    "    \n",
    "    print(f\"Training complete. Best accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60f77046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRecognizer:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = TextRecognitionModel()\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "        self.char_map = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        self.common_words = [\"the\", \"author\", \"text\", \"sample\", \"handwriting\"]\n",
    "\n",
    "    def recognize_text(self, image_path):\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('L')\n",
    "            img = self._preprocess_image(img)\n",
    "            char_images = self._segment_characters(img)\n",
    "            \n",
    "            raw_result = []\n",
    "            for char_img in char_images:\n",
    "                tensor = self.transform(char_img).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(tensor)\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                    raw_result.append(self.char_map[pred.item()])\n",
    "            \n",
    "            # Advanced word correction\n",
    "            word = ''.join(raw_result)\n",
    "            return self._correct_word(word)\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def _correct_word(self, word):\n",
    "        # Check against common words with fuzzy matching\n",
    "        for vocab_word in self.common_words:\n",
    "            if len(word) != len(vocab_word):\n",
    "                continue\n",
    "            matching_chars = sum(1 for a,b in zip(word, vocab_word) if a == b)\n",
    "            if matching_chars >= len(word) - 1:  # Allow 1 character error\n",
    "                return vocab_word\n",
    "        return word  # Fallback\n",
    "\n",
    "    def _preprocess_image(self, img):\n",
    "        img = np.array(img)\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Advanced binarization\n",
    "        thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                     cv2.THRESH_BINARY_INV, 21, 10)\n",
    "        return Image.fromarray(thresh)\n",
    "\n",
    "    def _segment_characters(self, img):\n",
    "        img_arr = np.array(img)\n",
    "        # Find contours with better parameters\n",
    "        contours, _ = cv2.findContours(img_arr, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        char_images = []\n",
    "        for cnt in sorted(contours, key=lambda x: cv2.boundingRect(x)[0]):\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            if w * h < 30:  # Filter very small noise\n",
    "                continue\n",
    "                \n",
    "            # Split connected characters using projection profiles\n",
    "            if w > h * 1.8:\n",
    "                vertical_proj = np.sum(img_arr[y:y+h, x:x+w], axis=0)\n",
    "                valleys = np.where(vertical_proj < np.mean(vertical_proj))[0]\n",
    "                if len(valleys) > 1:\n",
    "                    split_pos = valleys[len(valleys)//2]\n",
    "                    char_images.append(self._crop_char(img_arr, x, y, split_pos, h))\n",
    "                    char_images.append(self._crop_char(img_arr, x+split_pos, y, w-split_pos, h))\n",
    "                    continue\n",
    "            \n",
    "            char_images.append(self._crop_char(img_arr, x, y, w, h))\n",
    "        \n",
    "        return char_images\n",
    "    \n",
    "    def _crop_char(self, img, x, y, w, h):\n",
    "        size = max(w, h) + 24  # More padding\n",
    "        canvas = np.ones((size, size), dtype=np.uint8) * 255\n",
    "        start_x = (size - w) // 2\n",
    "        start_y = (size - h) // 2\n",
    "        canvas[start_y:start_y+h, start_x:start_x+w] = img[y:y+h, x:x+w]\n",
    "        return Image.fromarray(canvas)\n",
    "\n",
    "    def visualize_recognition(self, image_path):\n",
    "        result = self.recognize_text(image_path)\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.text(0.5, 0.5, f\"Recognized:\\n{result}\", \n",
    "                ha='center', va='center', fontsize=14, weight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb985ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generating 50 font samples...\n",
      "\n",
      "2. Generating exactly 60 handwritten samples...\n",
      "\n",
      "3. Training recognition model...\n",
      "Epoch 1, Loss: 3.7251, Accuracy: 2.80%\n",
      "Epoch 2, Loss: 3.5222, Accuracy: 3.15%\n",
      "Epoch 3, Loss: 3.4837, Accuracy: 6.64%\n",
      "Epoch 4, Loss: 3.3505, Accuracy: 6.29%\n",
      "Epoch 5, Loss: 3.1734, Accuracy: 5.59%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"1. Generating 50 font samples...\")\n",
    "    DatasetGenerator().generate_font_samples()\n",
    "    \n",
    "    print(\"\\n2. Generating exactly 60 handwritten samples...\")\n",
    "    DatasetGenerator().generate_handwriting_samples()\n",
    "    \n",
    "    print(\"\\n3. Training recognition model...\")\n",
    "    train_model()\n",
    "    \n",
    "    print(\"\\n4. Testing recognition system...\")\n",
    "    recognizer = TextRecognizer(\"text_recognition_model.pth\")\n",
    "    \n",
    "    test_image = \"your_test_image.png\"\n",
    "    if os.path.exists(test_image):\n",
    "        print(\"\\nResults:\")\n",
    "        recognizer.visualize_recognition(test_image)\n",
    "    else:\n",
    "        print(f\"\\nError: Save your test image as '{test_image}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83f045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
